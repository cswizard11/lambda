{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acQxE7E4O2Yx"
   },
   "source": [
    "# Unit 1 Sprint 3 Challenge - Linear Algebra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D90eph5lO5dp"
   },
   "source": [
    "## Welcome to the final sprint challenge of Unit 3!\n",
    "\n",
    "In this challenge, we're going to explore two different datasets where you can demonstrate your skills with fitting linear regression models and linear algebra.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* Load and explore the first dataset\n",
    "* Fit a simple linear regression model to one independent variable\n",
    "* Fit a regression model to two independent variables\n",
    "* Interpret R-squared and an adjusted R-squared values\n",
    "* Load word vector data and explore\n",
    "* Calculate the cosine similarity between the vectors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nC2VEo1cGaOD"
   },
   "source": [
    "### Data: Information\n",
    "\n",
    "This dataset is a record of seven common different fish species in fish market sales. With this dataset, we can practice fitting linear regression models. More information can be found at the link below.\n",
    "\n",
    "* [Kaggle: Fish Market Dataset](https://www.kaggle.com/aungpyaeap/fish-market)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9Ht0VS6GiM8"
   },
   "source": [
    "## 1) Load the data\n",
    "\n",
    "* From the URL provided, load the CSV file as a DataFrame - arguments: use the default parameters for `pandas.read_csv()`\n",
    "* The shape of your DataFrame should be `(159, 7)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "3YXv1PZQO5P9",
    "outputId": "3a47a129-f8bf-4ede-c75f-8c4d71ed17d0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1ba9fc8719ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# URL provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Fish/Fish.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "import pandas as pd\n",
    "# URL provided\n",
    "url = \"https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_1/Fish/Fish.csv\"\n",
    "\n",
    "### your code here ###\n",
    "df = pd.read_csv(url, na_values=0)\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVQcvdcuLgNf"
   },
   "source": [
    "## 2) Explore, Visualize, and Clean\n",
    "\n",
    "Now you want to take a look at the dataset, figure out what variable types the columns contain, identify missing values, and possibly look for outliers.\n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "* Use describe() and info() to learn about any missing values, the data types, and descriptive statistics for each numeric value\n",
    "\n",
    "*Hint: look at the minimum values for each column - would it make sense to remove rows that have a `0` in any of the columns?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Ks9ggzPSSLHv",
    "outputId": "850df073-ad2c-487f-bd97-3ecbabaf6dbf"
   },
   "outputs": [],
   "source": [
    "### your code here ###\n",
    "df.describe()\n",
    "\n",
    "df.isnull().sum()\n",
    "df = df.dropna()\n",
    "df.isnull().sum()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fongqoI9SZje"
   },
   "source": [
    "## 3) Visualize and Identify the Target\n",
    "\n",
    "Now that you know more about the data, we need to identify the *target* or response variable: what are we trying to predict? You can create a visualization to look at all of the numeric variables.\n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "* Use the seaborn library `sns.pairplot()` function to create your visualization (use the starter code)\n",
    "* Using the pairplot, identify two variables that you will use to fit a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EvY3PH7sRTLu",
    "outputId": "b7770892-5d5b-4120-bee7-95174995dd43"
   },
   "outputs": [],
   "source": [
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Use sns.pairplot(data) where data is the name of your DataFrame\n",
    "# sns.pairplot()\n",
    "\n",
    "## your code here ##\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RIZSLLjcOzj"
   },
   "source": [
    "### Short answer:\n",
    "\n",
    "I chose height as the independent, and weight as the dependent, since height is easy for someone to measure, and weight is what's important when selling at a fish market.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eq-Yh-MQY-k1"
   },
   "source": [
    "## 4) Create a scatter plot with a best-fit line\n",
    "\n",
    "Before we fit the linear regression model, we'll check how well a line fits our two variables. Because you have some choices for which independent variable to select, we're going to complete the rest of our analysis using `Width` as the independent variable and `Weight` as the dependent (target) variable.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "* Create a scatter plot using seaborn with `Width` and `Weight`\n",
    "* Use the sns.lmplot() and specify a confidence interval of 0.95\n",
    "* Answer the questions about your plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "oyCzMaHvalFo",
    "outputId": "79164f96-62c3-4ebd-fa5e-a7f1f2b37348"
   },
   "outputs": [],
   "source": [
    "## your code here ##\n",
    "ax = sns.lmplot(x='Width', y='Weight', data=df, ci=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tA0PT20JcUFj"
   },
   "source": [
    "### Short answer:\n",
    "\n",
    "1. Does it make sense to fit a linear model to these two variables? In otherwords, are there any problems with this data like extreme outliers, non-linearity, etc. - I think it makes sense. This set is mostly linear, and there are only a couple of outliers.\n",
    "2. Over what range of your independent variable does the model not fit the data well? - It doesn't fit great between 3 and 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvPJjQVccfa9"
   },
   "source": [
    "## 5) Fit a linear regression model\n",
    "\n",
    "Now it's time to fit the linear regression model! We have two variables (`width` and `weight`).\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "*  Use the `statsmodels.formula.api` library and import the `ols` method\n",
    "*  Fit a **single variable linear regression model** and print out the model summary\n",
    "*  Answer the questions about your resulting model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "SjxV58yUM-xM",
    "outputId": "fb65aebc-ed05-4bba-a08e-244986d769b0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f8c8d891680a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## your code here ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Weight ~ Width'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "## your code here ##\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "model = ols('Weight ~ Width', data=df).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cso00X7mNloI"
   },
   "source": [
    "### Short Answer:\n",
    "1. Is the correlation between your variables positive or negative? - Positive\n",
    "2. How would you write the confidence interval for your slope coefficient? - We are 95% confident that a change of 1 unit of width will change the weight by between 172.287g and 203.263g.\n",
    "3. State the null hypothesis to test for a statistically significant relationship between your two variables. - \n",
    "  \n",
    "  Ho: $\\beta_1$ = 0\n",
    "\n",
    "  Ha: $\\beta_1 \\neq$ 0\n",
    "4. Using the P value from your model, do you **reject** or **fail to reject** the null hypothesis? - Since the p-value of 0 is less than 0.05, we can reject the null hypothesis and conclude that there is a statistically significant relationship between width and weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMrxuOaHSFEc"
   },
   "source": [
    "## 6) Fit a multiple predictor linear regression model\n",
    "\n",
    "For this next task, we'll add in an additional independent or predictor variable. Let's look back at the pairplot and choose another variable - we'll use `Length1`.\n",
    "\n",
    "Now fit linear regression model using two predictor variables: `Width` and `Length1`.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "* Select an additional predictor variable\n",
    "* Fit a model with both variables and print out the model summary\n",
    "* Answer the questions about your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "Nu3XGDFqb8RT",
    "outputId": "e0e9f400-e985-402d-9487-b1a0d4ba4c4f"
   },
   "outputs": [],
   "source": [
    "## your code here ##\n",
    "model2 = ols('Weight ~ Width + Length1', data=df).fit()\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMVYt9vUeQgq"
   },
   "source": [
    "### Short answer:\n",
    "\n",
    "1. What percentage of the variance is explained by your first variable (when you only had one predictor variable)? - 78.5%\n",
    "2. How does the adjusted R-squared value change when a second pedictor variable is added? - It changes to 87.2%, which is an increase of 8.7%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfZxg7Xof5dj"
   },
   "source": [
    "## Part 2: Vectors and cosine similarity\n",
    "\n",
    "In this part of the challenge, we're going to look at how similar two vectors are. Remember, we can calculate the **cosine similarity** between two vectors by using this equation:\n",
    "\n",
    "$$\\cos \\theta= \\frac{\\mathbf {A} \\cdot \\mathbf {B} }{\\left\\|\\mathbf {A} \\right\\|\\left\\|\\mathbf {B} \\right\\|}$$\n",
    "\n",
    "$\\qquad$\n",
    "\n",
    "The numerator is the dot product of the vectors $\\mathbf {A}$ and $\\mathbf {B}$\n",
    "\n",
    "The denominator is the norm of $\\mathbf {A}$ times the norm of $\\mathbf {B}$\n",
    "\n",
    "### Three documents, two authors\n",
    "\n",
    "For this task, you will calculate the cosine similarity between three vectors. But here's the interesting part: each vector represents a \"chunk\" of text from a novel (a few chaptes of text). This text was cleaned to remove non-alphanumeric characters and numbers and then each document was transformed into a vector representation as described below. \n",
    "\n",
    "### Document vectors\n",
    "\n",
    "In the dataset you are going to load below, each row represents a word that occurs in at least one of the documents. So all the rows are all the words that are in our three documents.\n",
    "\n",
    "Each column represents a document (doc0, doc1, doc2). Now the fun part: the value in each cell is how frequently that word (row) occurs in that document (term-frequency) divided by how many documents that words appears in (document-frequency).\n",
    "\n",
    "`cell value = term_frequency / document_frequency`\n",
    "\n",
    "### Using cosine similarity\n",
    "\n",
    "You will be using cosine similarity to compare each document vector to the others. Remember that there are three documents, but two authors. Your task is to use the cosine similarity calculations to determine which two document vectors are most similar (written by the same author).\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "* Load in the csv file that contains the document vectors (this is coded for you - just run the cell)\n",
    "* Look at the DataFrame you just loaded in any way that helps you understand the format, what's included in the data, the shape of the DataFrame, etc.\n",
    "* Calculate the cosine similarity for **three pairs of vectors**\n",
    "  * doc0-doc1\n",
    "  * doc0-doc2\n",
    "  * doc1-doc2\n",
    "\n",
    "* Print out the results so you can refer to them\n",
    "* Answer the questions after you have completed the cosine similarity calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHcKj5epb8UQ"
   },
   "outputs": [],
   "source": [
    "# Imports (import pandas if you haven't)\n",
    "\n",
    "# Load the data - DON'T DELETE THIS CELL\n",
    "url = 'https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_4/unit1_nlp/text_vectors.csv'\n",
    "text = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "6orkQNkvyleB",
    "outputId": "227fe128-1442-4aaa-800d-d8a6d6f563d8"
   },
   "outputs": [],
   "source": [
    "## Explore the data\n",
    "\n",
    "## your code here ##\n",
    "print(text.isnull().sum())\n",
    "text.columns = ['blank', 'word', 'doc0', 'doc1', 'doc2']\n",
    "text_new = text.drop('blank', axis=1)\n",
    "print(text_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "AgTYYDTbzDnd",
    "outputId": "6afaa172-57f1-45fe-c874-6190936c3828"
   },
   "outputs": [],
   "source": [
    "## Calculate the cosine similarity\n",
    "\n",
    "# Use these imports for you cosine calculations (DON'T DELETE)\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "## your code here ##\n",
    "\n",
    "cos_sim01 = dot(text_new['doc0'], text_new['doc1'])/(norm(text_new['doc0'])*norm(text_new['doc1']))\n",
    "cos_sim02 = dot(text_new['doc0'], text_new['doc2'])/(norm(text_new['doc0'])*norm(text_new['doc2']))\n",
    "cos_sim12 = dot(text_new['doc1'], text_new['doc2'])/(norm(text_new['doc1'])*norm(text_new['doc2']))\n",
    "print(cos_sim01)\n",
    "print(cos_sim02)\n",
    "print(cos_sim12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eS4D8WD0_xe"
   },
   "source": [
    "## Short answer\n",
    "\n",
    "1. Using your cosine similarity calculations, which two documents are most similar? - Document 1 and 2 are most similar.\n",
    "2. If doc1 and doc2 were written by the same author, are your cosine similarity calculations consistent with this statement? - Yes.\n",
    "3. What process would we need to follow to add an additional document column? In other words, why can't we just stick another column with (term-frequency/document-frequency) values onto our current DataFrame `text`? - Since document-frequency is dependent on the number of documents, and it's used to caluclate each cell, adding a document would change document-frequency, thereby requiring us to modify every cell in the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8Foxa3p2an1"
   },
   "source": [
    "**Additional Information about the texts used in this analysis:**\n",
    "\n",
    "You can find the raw text [here](https://github.com/LambdaSchool/data-science-practice-datasets/tree/main/unit_4/unit1_nlp). Dcoument 0 (doc0) is chapters 1-3 from \"Pride and Predjudice\" by Jane Austen. Document 1 (doc1) is chapters 1- 4 from \"Frankenstein\" by Mary Shelley. Document 2 is also from \"Frankenstein\", chapters 11-14."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_Unit1_Sprint3_Challenge.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
